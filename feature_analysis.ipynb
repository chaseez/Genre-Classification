{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It makes sense to drop track_id artist_name and title\n",
    "\n",
    "# TODO\n",
    "# - Normalization/Standardization of most if not all of these features\n",
    "# - Figure out what do do with time_signature, key and mode\n",
    "# - Pop and Classic rock have 15,000 entries while Rap/Hip hop only has 500 or so, we will either need to downsample\n",
    "#   or else make use of synthetic minority oversampling\n",
    "# - Note that loudness has some negative values. This may affect our approach for normalization. We may also need to keep in mind that\n",
    "#   this is probably measured in decibels and is thus logarithmic. \n",
    "#   I believe this won't be a problem as the network can learn non linear relationships\n",
    "\n",
    "# - I will try twice. One dropping the key attribute, one time with it. On a purely musical level the key doesn't mattert\n",
    "#   but it's possible that certain genres use one key more often, and it's possible that the timbre measurements could have some synergy\n",
    "#   with the key in play.\n",
    "\n",
    "\n",
    "df = pd.read_csv('msd_genre_dataset.txt')\n",
    "df_relevant_features = df.drop(columns=[\"track_id\", \"artist_name\", \"title\"])\n",
    "df_relevant_features.head()\n",
    "\n",
    "X = df_relevant_features.drop(columns=['genre'])\n",
    "# X = df_relevent_features.drop(columns=['key'])\n",
    "\n",
    "X = pd.get_dummies(X, columns=['time_signature', 'key', 'mode'])\n",
    "# X = X.get_dummies(X, columns=['time_signature', 'mode'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_relevant_features['genre'])\n",
    "\n",
    "# Now the data is properly encoded and irrelevant variables are gone\n",
    "\n",
    "# Normalize the features\n",
    "# We should experiment with a few types of normalization/standardization, some features are more uniform than others\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert normalized features back to a DataFrame for easier inspection\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.14998559 0.07696145 0.04822348 0.03780027 0.03670179 0.03365263\n",
      " 0.03039367 0.02920828 0.02461011 0.02408281]\n",
      "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0 -3.061145  0.299252 -0.727100  0.814719  2.128445  0.111168 -1.344552   \n",
      "1 -1.822265 -0.423075  1.995744 -0.788603  2.403267  1.409285 -0.385317   \n",
      "2  0.952322 -1.000054  1.322135 -1.506617  2.967055 -0.219142  0.386567   \n",
      "3 -3.126369 -0.695175 -0.981982  0.641821 -0.508449  0.157359 -0.187894   \n",
      "4 -2.993877 -1.167106  2.528458 -0.087107 -0.150061 -0.105525 -0.063881   \n",
      "\n",
      "        PC8       PC9      PC10  \n",
      "0  0.201114  0.677558  0.568962  \n",
      "1 -1.008940  0.100014 -0.432697  \n",
      "2 -0.269162  2.449830 -0.086100  \n",
      "3 -0.454608 -0.254903 -2.093109  \n",
      "4 -0.670802  0.586568  0.484102  \n"
     ]
    }
   ],
   "source": [
    "##### PCA #####\n",
    "# Initialize PCA with the desired number of components\n",
    "pca = PCA(n_components=10)  # Adjust the number of components as needed\n",
    "\n",
    "# Apply PCA to the normalized features\n",
    "X_pca = pca.fit_transform(X_normalized)\n",
    "\n",
    "# Convert PCA results back to a DataFrame for easier inspection\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the DataFrame with PCA results\n",
    "print(X_pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5874161073825503\n"
     ]
    }
   ],
   "source": [
    "### Let's Train This Thing ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, I know there's some better metrics we can use right?\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takeaways ###\n",
    "\n",
    "# With a PCA of ten PCs, and an MLP with 100 and 50 layers, we got 50% test accuracy\n",
    "# W/o PCA we got nearly 60%\n",
    "# I bet the low accuracy has something to do with the skewed data, that's probably our next step!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (YourEnvironmentName)",
   "language": "python",
   "name": "yourenvironmentname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
